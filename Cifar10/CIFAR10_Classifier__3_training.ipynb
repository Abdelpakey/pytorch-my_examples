{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR10 Classifier. Training\n",
    "\n",
    "\n",
    "This is the third part of the tutorial on how to train a classifier on CIFAR10 dataset. \n",
    "\n",
    "Here we will assemble the results of two previous parts and train a model on CIFAR10.\n",
    "\n",
    "- Setup dataflow\n",
    "- Setup model: SqueezeNet v1.1\n",
    "- Setup loss function and optimizer\n",
    "- Setup training pipeline\n",
    "    - Training\n",
    "    - Inference\n",
    "\n",
    "References:\n",
    "- [pytorch-examples/imagenet](https://github.com/pytorch/examples/blob/master/imagenet/main.py)\n",
    "- [pytorch-transfer learning](http://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html#sphx-glr-beginner-transfer-learning-tutorial-py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://ipython.org/ipython-doc/3/config/extensions/autoreload.html\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['CIFAR10_ROOT'] = '/media/user/fast_storage/tensorpack_data/cifar10_data/'\n",
    "os.environ['CIFAR10_ROOT'] = '/Users/vfomin/tensorpack_data/cifar10_data/'\n",
    "CIFAR10_ROOT = os.environ['CIFAR10_ROOT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup dataflow\n",
    "\n",
    "Again, we have two CIFAR10 datasets: `training` and `testing`. Next, we :\n",
    "- separate training dataset using stratified split into `n` folds of `training` and `validation` datasets.\n",
    "- apply data augmentations\n",
    "- gather data in batches\n",
    "- all previous operations using multiprocessing\n",
    "- load on GPU\n",
    "\n",
    "We won't iterate over folds and consider only the first fold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, Normalize, ToTensor, Lambda\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from common_utils.dataflow import TransformedDataset, OnGPUDataLoader\n",
    "from common_utils.imgaug import ToNumpy, RandomOrder, RandomChoice, RandomFlip, RandomAffine, ColorJitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw datasets: training and testing\n",
    "train_ds = torchvision.datasets.CIFAR10(root=CIFAR10_ROOT, train=True, download=False)\n",
    "test_ds = torchvision.datasets.CIFAR10(root=CIFAR10_ROOT, train=False, download=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's again resize input images to 42x42. Most of common state-of-the-art architectures requires input images larger than 224x224. Smaller input images will produce zero-size feature maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ResizeDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, ds, output_size=(32, 32)):        \n",
    "        assert isinstance(ds, Dataset)        \n",
    "        self.ds = ds\n",
    "        self.output_size = output_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.ds[index]\n",
    "        x = x.resize(self.output_size)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "resized_train_ds = ResizeDataset(train_ds, output_size=(42, 42))\n",
    "resized_test_ds = ResizeDataset(test_ds, output_size=(42, 42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "n_samples = len(resized_train_ds)\n",
    "X = np.zeros(n_samples)\n",
    "Y = np.zeros(n_samples)\n",
    "for i, (_, label) in enumerate(resized_train_ds):\n",
    "    Y[i] = label\n",
    "\n",
    "kfolds_train_indices = []\n",
    "kfolds_val_indices = []\n",
    "    \n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=12345)\n",
    "for train_indices, val_indices in skf.split(X, Y):\n",
    "    kfolds_train_indices.append(train_indices)\n",
    "    kfolds_val_indices.append(val_indices)  \n",
    "    \n",
    "kfold_samplers = []\n",
    "for train_indices, val_indices in zip(kfolds_train_indices, kfolds_val_indices):\n",
    "    kfold_samplers.append({\"train\": SubsetRandomSampler(train_indices), \n",
    "                           \"val\": SubsetRandomSampler(val_indices)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentations :\n",
    "# following pytorch.org/docs/master/torchvision/models.html\n",
    "mean_val = [0.485, 0.456, 0.406]\n",
    "std_val = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Setup data augmentations\n",
    "train_transforms = Compose([\n",
    "    ToNumpy(),\n",
    "    # Geometry\n",
    "    RandomChoice([\n",
    "        RandomAffine(rotation=(-60, 60), scale=(0.95, 1.05), translate=(0.05, 0.05)),\n",
    "        RandomFlip(proba=0.5, mode='h'),\n",
    "        RandomFlip(proba=0.5, mode='v'),        \n",
    "    ]),    \n",
    "    # To Tensor (float, CxHxW, [0.0, 1.0]) + Normalize\n",
    "    ToTensor(),\n",
    "    # Color\n",
    "    ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
    "    Normalize(mean_val, std_val)\n",
    "])\n",
    "  \n",
    "\n",
    "test_transforms = Compose([\n",
    "    ToNumpy(),    \n",
    "    # Geometry\n",
    "    RandomChoice([\n",
    "        RandomAffine(rotation=(-60, 60), scale=(0.95, 1.05), translate=(0.05, 0.05)),\n",
    "        RandomFlip(proba=0.5, mode='h'),\n",
    "        RandomFlip(proba=0.5, mode='v'),        \n",
    "    ]),        \n",
    "    # To Tensor (float, CxHxW, [0.0, 1.0])  + Normalize\n",
    "    ToTensor(),\n",
    "    # Color\n",
    "    ColorJitter(brightness=0.15, contrast=0.15, saturation=0.15),\n",
    "    Normalize(mean_val, std_val)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_aug_train_ds = TransformedDataset(resized_train_ds, x_transforms=train_transforms)\n",
    "data_aug_val_ds = TransformedDataset(resized_train_ds, x_transforms=test_transforms)\n",
    "data_aug_test_ds = TransformedDataset(resized_test_ds, x_transforms=test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_index = 0 \n",
    "\n",
    "_DataLoader = OnGPUDataLoader if torch.cuda.is_available() else DataLoader\n",
    "    \n",
    "    \n",
    "train_batches_ds = _DataLoader(data_aug_train_ds, \n",
    "                               batch_size=64, \n",
    "                               sampler=kfold_samplers[split_index][\"train\"], \n",
    "                               num_workers=0, \n",
    "                               drop_last=True, \n",
    "                               pin_memory=False)\n",
    "\n",
    "val_batches_ds = _DataLoader(data_aug_val_ds, \n",
    "                             batch_size=64, \n",
    "                             sampler=kfold_samplers[split_index][\"val\"], \n",
    "                             num_workers=0, \n",
    "                             drop_last=True, \n",
    "                             pin_memory=False)\n",
    "\n",
    "test_batches_ds = _DataLoader(data_aug_test_ds, \n",
    "                              batch_size=64, \n",
    "                              num_workers=0, \n",
    "                              drop_last=True, \n",
    "                              pin_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check again training classes distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (63) : OS call failed or operation not supported on this OS at /Users/vfomin/Documents/ML/pytorch-master_source/aten/src/THC/THCCachingHostAllocator.cpp:257",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-730debdcc40c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# pbar = tqdm(total=len(train_batches_ds))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batches_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mclasses_stats_per_batches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ML/pytorch-my_examples/common_utils/dataflow.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_pinned\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                 \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0mcuda_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masync\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcuda_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mpin_memory\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstorage\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshare_memory_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/torch/storage.py\u001b[0m in \u001b[0;36mpin_memory\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mallocator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_host_allocator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallocator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallocator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshare_memory_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (63) : OS call failed or operation not supported on this OS at /Users/vfomin/Documents/ML/pytorch-master_source/aten/src/THC/THCCachingHostAllocator.cpp:257"
     ]
    }
   ],
   "source": [
    "n_classes = 10\n",
    "mean_n_classes = []\n",
    "std_n_classes = []\n",
    "cnt = 0\n",
    "\n",
    "classes_stats_per_batches = np.zeros((len(train_batches_ds), n_classes), dtype=np.int)\n",
    "\n",
    "pbar = tqdm(total=len(train_batches_ds))\n",
    "for i, (batch_x, batch_y) in enumerate(train_batches_ds):\n",
    "    for y in batch_y:\n",
    "        classes_stats_per_batches[i, y] += 1\n",
    "\n",
    "    postfix_str = \"c0: %i | c1: %i\" % (classes_stats_per_batches[i, 0], classes_stats_per_batches[i, 1])\n",
    "    pbar.set_postfix_str(postfix_str, refresh=True)\n",
    "    pbar.update(1)    \n",
    "    \n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=classes_stats_per_batches, palette=\"PRGn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x13c119f98>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe0AAAFJCAYAAAC2OXUDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQbQBADCCaAMAYMT/AhcufszjVxB0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13c0b3400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean_n_classes = np.mean(np.cumsum(classes_stats_per_batches, axis=0), axis=1)\n",
    "std_n_classes = np.std(np.cumsum(classes_stats_per_batches, axis=0), axis=1)\n",
    "\n",
    "plt.plot(mean_n_classes)\n",
    "plt.plot(mean_n_classes + 3.0 * std_n_classes)\n",
    "plt.plot(mean_n_classes - 3.0 * std_n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup model\n",
    "\n",
    "Let's load a small neural network \"SqueezeNet\" that has only ~700K parameters, showing performances similar to AlexNet:\n",
    "- Top-1 ImageNet Accuracy: 57.5% vs 57.2% (AlexNet)\n",
    "- Top-5 ImageNet Accuracy: 80.3% vs 80.3% (AlexNet)\n",
    "\n",
    "References:\n",
    "- [paper](https://arxiv.org/pdf/1602.07360.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import SqueezeNet\n",
    "\n",
    "from common_utils.nn_utils import print_trainable_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SqueezeNet (\n",
      "  (features): Sequential (\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2))\n",
      "    (1): ReLU (inplace)\n",
      "    (2): MaxPool2d (size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
      "    (3): Fire (\n",
      "      (squeeze): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (4): Fire (\n",
      "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (5): MaxPool2d (size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
      "    (6): Fire (\n",
      "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (7): Fire (\n",
      "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (8): MaxPool2d (size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
      "    (9): Fire (\n",
      "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (10): Fire (\n",
      "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (11): Fire (\n",
      "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (12): Fire (\n",
      "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential (\n",
      "    (0): Dropout (p = 0.5)\n",
      "    (1): Conv2d(512, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (2): ReLU (inplace)\n",
      "    (3): AvgPool2d (size=13, stride=13, padding=0, ceil_mode=False, count_include_pad=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "squeezenet = SqueezeNet(num_classes=10, version=1.1)\n",
    "print(squeezenet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0.weight torch.Size([64, 3, 3, 3])\n",
      "features.0.bias torch.Size([64])\n",
      "features.3.squeeze.weight torch.Size([16, 64, 1, 1])\n",
      "features.3.squeeze.bias torch.Size([16])\n",
      "features.3.expand1x1.weight torch.Size([64, 16, 1, 1])\n",
      "features.3.expand1x1.bias torch.Size([64])\n",
      "features.3.expand3x3.weight torch.Size([64, 16, 3, 3])\n",
      "features.3.expand3x3.bias torch.Size([64])\n",
      "features.4.squeeze.weight torch.Size([16, 128, 1, 1])\n",
      "features.4.squeeze.bias torch.Size([16])\n",
      "features.4.expand1x1.weight torch.Size([64, 16, 1, 1])\n",
      "features.4.expand1x1.bias torch.Size([64])\n",
      "features.4.expand3x3.weight torch.Size([64, 16, 3, 3])\n",
      "features.4.expand3x3.bias torch.Size([64])\n",
      "features.6.squeeze.weight torch.Size([32, 128, 1, 1])\n",
      "features.6.squeeze.bias torch.Size([32])\n",
      "features.6.expand1x1.weight torch.Size([128, 32, 1, 1])\n",
      "features.6.expand1x1.bias torch.Size([128])\n",
      "features.6.expand3x3.weight torch.Size([128, 32, 3, 3])\n",
      "features.6.expand3x3.bias torch.Size([128])\n",
      "features.7.squeeze.weight torch.Size([32, 256, 1, 1])\n",
      "features.7.squeeze.bias torch.Size([32])\n",
      "features.7.expand1x1.weight torch.Size([128, 32, 1, 1])\n",
      "features.7.expand1x1.bias torch.Size([128])\n",
      "features.7.expand3x3.weight torch.Size([128, 32, 3, 3])\n",
      "features.7.expand3x3.bias torch.Size([128])\n",
      "features.9.squeeze.weight torch.Size([48, 256, 1, 1])\n",
      "features.9.squeeze.bias torch.Size([48])\n",
      "features.9.expand1x1.weight torch.Size([192, 48, 1, 1])\n",
      "features.9.expand1x1.bias torch.Size([192])\n",
      "features.9.expand3x3.weight torch.Size([192, 48, 3, 3])\n",
      "features.9.expand3x3.bias torch.Size([192])\n",
      "features.10.squeeze.weight torch.Size([48, 384, 1, 1])\n",
      "features.10.squeeze.bias torch.Size([48])\n",
      "features.10.expand1x1.weight torch.Size([192, 48, 1, 1])\n",
      "features.10.expand1x1.bias torch.Size([192])\n",
      "features.10.expand3x3.weight torch.Size([192, 48, 3, 3])\n",
      "features.10.expand3x3.bias torch.Size([192])\n",
      "features.11.squeeze.weight torch.Size([64, 384, 1, 1])\n",
      "features.11.squeeze.bias torch.Size([64])\n",
      "features.11.expand1x1.weight torch.Size([256, 64, 1, 1])\n",
      "features.11.expand1x1.bias torch.Size([256])\n",
      "features.11.expand3x3.weight torch.Size([256, 64, 3, 3])\n",
      "features.11.expand3x3.bias torch.Size([256])\n",
      "features.12.squeeze.weight torch.Size([64, 512, 1, 1])\n",
      "features.12.squeeze.bias torch.Size([64])\n",
      "features.12.expand1x1.weight torch.Size([256, 64, 1, 1])\n",
      "features.12.expand1x1.bias torch.Size([256])\n",
      "features.12.expand3x3.weight torch.Size([256, 64, 3, 3])\n",
      "features.12.expand3x3.bias torch.Size([256])\n",
      "classifier.1.weight torch.Size([10, 512, 1, 1])\n",
      "classifier.1.bias torch.Size([10])\n",
      "\n",
      "Total number of trainable parameters:  727626\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(squeezenet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put the model on GPU if CUDA available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    squeezenet = squeezenet.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we work with resized CIFAR10 images of size 42 x 42, we replace first layers that drastically reduce feature map size and adapt the last average pooling layer in order to avoid zero size feature maps. \n",
    "\n",
    "Let's see feature map sizes coming from `squeezenet.features`:\n",
    "- from the first layers before the first 'fire' module: \n",
    "    - 0: Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2))\n",
    "    - 1: ReLU (inplace)\n",
    "    - 2: MaxPool2d (size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
    "- the last layer before classification part:\n",
    "    - 12: Fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import AvgPool2d, Sequential, MaxPool2d, Conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2)),\n",
       " MaxPool2d (size=(3, 3), stride=(2, 2), dilation=(1, 1)))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squeezenet.features[0], squeezenet.features[2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 64, 23, 23]), torch.Size([1, 64, 11, 11]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "test_random_x = Variable(torch.randn(1, 3, 48, 48))\n",
    "if torch.cuda.is_available():\n",
    "    test_random_x.data = test_random_x.data.pin_memory()\n",
    "    test_random_x = test_random_x.cuda(async=True)\n",
    "    \n",
    "squeezenet.eval()\n",
    "test_output_y0 = squeezenet.features[0](test_random_x)\n",
    "test_output_y1 = Sequential(squeezenet.features[0], squeezenet.features[1], squeezenet.features[2])(test_random_x)\n",
    "test_output_y0.size(), test_output_y1.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's replace first layers : \n",
    "- `Conv2d (3, 64, kernel_size=(3, 3), stride=(2, 2))` by `Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=1)`\n",
    "- remove `MaxPool2d (size=(3, 3), stride=(2, 2), dilation=(1, 1)))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [l for i, l in enumerate(squeezenet.features) if i != 2]\n",
    "layers[0] = Conv2d(3, 64, kernel_size=(3, 3), padding=1)\n",
    "\n",
    "squeezenet.features = Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SqueezeNet (\n",
      "  (features): Sequential (\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU (inplace)\n",
      "    (2): Fire (\n",
      "      (squeeze): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (3): Fire (\n",
      "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (4): MaxPool2d (size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
      "    (5): Fire (\n",
      "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (6): Fire (\n",
      "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (7): MaxPool2d (size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
      "    (8): Fire (\n",
      "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (9): Fire (\n",
      "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (10): Fire (\n",
      "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (11): Fire (\n",
      "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential (\n",
      "    (0): Dropout (p = 0.5)\n",
      "    (1): Conv2d(512, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (2): ReLU (inplace)\n",
      "    (3): AvgPool2d (size=13, stride=13, padding=0, ceil_mode=False, count_include_pad=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(squeezenet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    squeezenet = squeezenet.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 10, 10])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "test_random_x = Variable(torch.randn(1, 3, 42, 42))\n",
    "if torch.cuda.is_available():\n",
    "    test_random_x.data = test_random_x.data.pin_memory()\n",
    "    test_random_x = test_random_x.cuda(async=True)\n",
    "\n",
    "squeezenet.eval()\n",
    "test_output_y = squeezenet.features(test_random_x)\n",
    "test_output_y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import AvgPool2d, Sequential\n",
    "\n",
    "layers = [l for l in squeezenet.classifier]\n",
    "layers[-1] = AvgPool2d(10)\n",
    "\n",
    "squeezenet.classifier = Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SqueezeNet (\n",
      "  (features): Sequential (\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU (inplace)\n",
      "    (2): Fire (\n",
      "      (squeeze): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (3): Fire (\n",
      "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (4): MaxPool2d (size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
      "    (5): Fire (\n",
      "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (6): Fire (\n",
      "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (7): MaxPool2d (size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
      "    (8): Fire (\n",
      "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (9): Fire (\n",
      "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (10): Fire (\n",
      "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (11): Fire (\n",
      "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential (\n",
      "    (0): Dropout (p = 0.5)\n",
      "    (1): Conv2d(512, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (2): ReLU (inplace)\n",
      "    (3): AvgPool2d (size=10, stride=10, padding=0, ceil_mode=False, count_include_pad=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(squeezenet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from previous prints, the last layer of the model is average pooling and the output of the forward pass is logits and not yet probabilities of classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.0050  0.0027  0.1234  0.0081  0.0000  0.1569  0.0351  0.0587  0.0738  0.0419\n",
       "[torch.FloatTensor of size 1x10]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "test_random_x = Variable(torch.randn(1, 3, 48, 48))\n",
    "if torch.cuda.is_available():\n",
    "    test_random_x.data = test_random_x.data.pin_memory()\n",
    "    test_random_x = test_random_x.cuda(async=True)\n",
    "\n",
    "squeezenet.eval()\n",
    "test_output_y = squeezenet(test_random_x)\n",
    "test_output_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup loss function and optimizer\n",
    "\n",
    "Let's choose classical cross-entropy loss function for this multiclass classification task and Adam as an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function (criterion) and optimizer\n",
    "criterion = CrossEntropyLoss()\n",
    "if torch.cuda.is_available():\n",
    "    criterion = CrossEntropyLoss().cuda()\n",
    "optimizer = Adam(squeezenet.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that CrossEntropyLoss:\n",
    "\n",
    "> This criterion combines `LogSoftMax` and `NLLLoss` in one single class.\n",
    "> It is useful when training a classification problem with `n` classes.\n",
    "\n",
    "> The `input` is expected to contain scores for each class.\n",
    "> `input` has to be a 2D `Tensor` of size `batch x n`.\n",
    "\n",
    "> This criterion expects a class index (0 to nClasses-1) as the\n",
    "> `target` for each value of a 1D tensor of size `n`\n",
    "\n",
    ">    The loss can be described as:\n",
    "$$\n",
    "        loss(x, class) = -log(exp(x[class]) / (\\sum_j exp(x[j])))\n",
    "                       = -x[class] + log(\\sum_j exp(x[j]))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check loss function computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss :  \n",
      " 2.3048\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "squeezenet.eval()\n",
    "\n",
    "for i, (batch_x, batch_y) in enumerate(train_batches_ds):\n",
    "    \n",
    "    batch_x = Variable(batch_x, requires_grad=True)\n",
    "    batch_y = Variable(batch_y)\n",
    "    batch_y_pred = squeezenet(batch_x)\n",
    "    loss = criterion(batch_y_pred, batch_y)\n",
    "    print(\"Loss : \", loss.data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup training pipeline\n",
    "\n",
    "Typical pipeline: \n",
    "- loops over training dataset during `n_epochs`\n",
    "    - computes values of the loss function, accuracy, other metrics on train batches\n",
    "- runs validation phase on validation dataset when training epoch ends\n",
    "    - computes values of the loss function, accuracy, other metrics on whole validation dataset\n",
    "- save on the disk the best model defined by a metric\n",
    "- performs learning rate scheduling on each training epoch\n",
    "\n",
    "\n",
    "Next, there are two ways:\n",
    "- copy/modify code from examples: i.e. [link](https://github.com/pytorch/examples/blob/master/imagenet/main.py)\n",
    "- use [torchsample](https://github.com/ncullen93/torchsample) or [tnt](https://github.com/pytorch/tnt) ...\n",
    "\n",
    "Here we choose the first way and explicitly code a training pipeline based on various available examples.\n",
    "Let's first define methods called in the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common_utils.training_utils import AverageMeter, accuracy, save_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, cuda_batches, criterion, optimizer, epoch, n_epochs):    \n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    \n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    pbar = tqdm(total=len(cuda_batches))\n",
    "    for i, (batch_x, batch_y) in enumerate(cuda_batches):\n",
    "\n",
    "        batch_x = Variable(batch_x)\n",
    "        batch_y = Variable(batch_y)\n",
    "\n",
    "        # compute output\n",
    "        batch_y_pred = model(batch_x)\n",
    "        loss = criterion(batch_y_pred, batch_y)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(batch_y_pred.data, batch_y.data, topk=(1, 5))\n",
    "        losses.update(loss.data[0], batch_x.size(0))\n",
    "        top1.update(prec1[0], batch_x.size(0))\n",
    "        top5.update(prec5[0], batch_x.size(0))\n",
    "\n",
    "        # compute gradient and do optimizer step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        prefix_str = \"Epoch: {}/{}\".format(epoch + 1, n_epochs)        \n",
    "        pbar.set_description_str(prefix_str, refresh=False)\n",
    "        \n",
    "        post_fix_str = 'Loss {loss.avg:.4f} | ' + \\\n",
    "                        'Prec@1 {top1.avg:.3f} | ' + \\\n",
    "                        'Prec@5 {top5.avg:.3f}'\n",
    "                        \n",
    "        post_fix_str = post_fix_str.format(loss=losses, top1=top1, top5=top5)\n",
    "        \n",
    "        pbar.set_postfix_str(post_fix_str, refresh=True)\n",
    "        pbar.update(1)\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, cuda_batches, criterion):\n",
    "    \n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    pbar = tqdm(total=len(cuda_batches))\n",
    "    for i, (batch_x, batch_y) in enumerate(cuda_batches):\n",
    "\n",
    "        batch_x = Variable(batch_x, volatile=True)  # volatile means that the Variable should be used in inference mode, i.e. don’t save the history.\n",
    "        batch_y = Variable(batch_y, volatile=True)   # see http://pytorch.org/docs/master/autograd.html#variable\n",
    "\n",
    "        # compute output\n",
    "        batch_y_pred = model(batch_x)\n",
    "        loss = criterion(batch_y_pred, batch_y)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(batch_y_pred.data, batch_y.data, topk=(1, 5))\n",
    "        losses.update(loss.data[0], batch_x.size(0))\n",
    "        top1.update(prec1[0], batch_x.size(0))\n",
    "        top5.update(prec5[0], batch_x.size(0))\n",
    "\n",
    "        pbar.set_description_str(\"Test\", refresh=False)\n",
    "        post_fix_str = 'Loss {loss.avg:.4f} | ' + \\\n",
    "                        'Prec@1 {top1.avg:.3f} | ' + \\\n",
    "                        'Prec@5 {top5.avg:.3f}'\n",
    "        post_fix_str = post_fix_str.format(loss=losses, top1=top1, top5=top5)\n",
    "        \n",
    "        pbar.set_postfix_str(post_fix_str, refresh=True)\n",
    "        pbar.update(1)\n",
    "    pbar.close()                        \n",
    "    return top1.avg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ExponentialLR, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we can start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0 \n",
    "n_epochs = 10\n",
    "model = squeezenet\n",
    "init_lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.backends import cudnn\n",
    "cudnn.benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What does torch.backends.cudnn.benchmark do? [url](https://discuss.pytorch.org/t/what-does-torch-backends-cudnn-benchmark-do/5936/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(squeezenet.parameters(), lr=init_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10:   2%|▏         | 14/625 [01:05<48:28,  4.76s/it, Loss 2.3026 | Prec@1 10.491 | Prec@5 50.558]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-ef098b658b0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# train for one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_batches_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# evaluate on validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-1967c38b7bab>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, cuda_batches, criterion, optimizer, epoch, n_epochs)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# compute output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mbatch_y_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_y_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/torchvision/models/squeezenet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/torchvision/models/squeezenet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand1x1_activation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand1x1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand3x3_activation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand3x3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         ], 1)\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mcat\u001b[0;34m(iterable, dim)\u001b[0m\n\u001b[1;32m    895\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 897\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mConcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/torch/autograd/_functions/tensor.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, dim, *inputs)\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "best_prec1 = 0\n",
    "now = datetime.now()\n",
    "\n",
    "if not os.path.exists('logs'):\n",
    "    os.makedirs('logs')\n",
    "    \n",
    "logs_path = os.path.join('logs', 'cifar10_squeezenet_%s' % now.strftime(\"%Y%m%d_%H%M\"))\n",
    "if not os.path.exists(logs_path):\n",
    "    os.makedirs(logs_path)    \n",
    "\n",
    "\n",
    "for epoch in range(start_epoch, n_epochs):\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "    # train for one epoch\n",
    "    train_one_epoch(model, train_batches_ds, criterion, optimizer, epoch, n_epochs)\n",
    "\n",
    "    # evaluate on validation set\n",
    "    prec1 = validate(model, val_batches_ds, criterion)\n",
    "\n",
    "    # remember best prec@1 and save checkpoint\n",
    "    is_best = prec1 > best_prec1\n",
    "    best_prec1 = max(prec1, best_prec1)\n",
    "    save_checkpoint(logs_path, {\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'val_prec1': prec1,\n",
    "        'best_prec1': best_prec1,\n",
    "        'optimizer' : optimizer.state_dict()}, is_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "Let's upload the best saved model and run inference on the testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common_utils.training_utils import load_checkpoint\n",
    "\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_filenames = glob(os.path.join(logs_path, \"model_val_prec1=*\"))\n",
    "assert len(saved_model_filenames) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_filename = saved_model_filenames[0]\n",
    "\n",
    "load_checkpoint(saved_model_filename, squeezenet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 156/156 [00:02<00:00, 52.31it/s, Loss 1.3473 | Prec@1 51.482 | Prec@5 93.199]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "51.482371794871796"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate on validation set\n",
    "prec1 = validate(squeezenet, test_batches_ds, criterion)\n",
    "prec1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
