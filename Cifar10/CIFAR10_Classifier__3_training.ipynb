{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR10 Classifier. Training\n",
    "\n",
    "\n",
    "This is the third part of the tutorial on how to train a classifier on CIFAR10 dataset. \n",
    "\n",
    "Here we will assemble the results of two previous parts and train a model on CIFAR10.\n",
    "\n",
    "- Setup dataflow\n",
    "- Setup model: SqueezeNet v1.1\n",
    "- Setup loss function and optimizer\n",
    "- Setup training pipeline\n",
    "    - Training\n",
    "    - Inference\n",
    "\n",
    "References:\n",
    "- [pytorch-examples/imagenet](https://github.com/pytorch/examples/blob/master/imagenet/main.py)\n",
    "- [pytorch-transfer learning](http://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html#sphx-glr-beginner-transfer-learning-tutorial-py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://ipython.org/ipython-doc/3/config/extensions/autoreload.html\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CIFAR10_ROOT'] = '/media/user/fast_storage/tensorpack_data/cifar10_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CIFAR10_ROOT = os.environ['CIFAR10_ROOT']\n",
    "sys.path.append(\"common\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup dataflow\n",
    "\n",
    "Again, we have two CIFAR10 datasets: `training` and `testing`. Next, we :\n",
    "- separate training dataset using stratified split into `n` folds of `training` and `validation` datasets.\n",
    "- apply data augmentations\n",
    "- gather data in batches\n",
    "- all previous operations using multiprocessing\n",
    "- load on GPU\n",
    "\n",
    "We won't iterate over folds and consider only the first fold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, Normalize, ToTensor, Lambda\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from common.dataflow import TransformedDataset, OnGPUDataLoader\n",
    "from common.imgaug import ToNumpy, RandomOrder, RandomChoice, RandomFlip, RandomAffine, ColorJitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw datasets: training and testing\n",
    "train_ds = torchvision.datasets.CIFAR10(root=CIFAR10_ROOT, train=True, download=False)\n",
    "test_ds = torchvision.datasets.CIFAR10(root=CIFAR10_ROOT, train=False, download=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's again resize input images to 42x42. Most of common state-of-the-art architectures requires input images larger than 224x224. Smaller input images will produce zero-size feature maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ResizeDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, ds, output_size=(32, 32)):        \n",
    "        assert isinstance(ds, Dataset)        \n",
    "        self.ds = ds\n",
    "        self.output_size = output_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.ds[index]\n",
    "        x = x.resize(self.output_size)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "resized_train_ds = ResizeDataset(train_ds, output_size=(42, 42))\n",
    "resized_test_ds = ResizeDataset(test_ds, output_size=(42, 42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "n_samples = len(resized_train_ds)\n",
    "X = np.zeros(n_samples)\n",
    "Y = np.zeros(n_samples)\n",
    "for i, (_, label) in enumerate(resized_train_ds):\n",
    "    Y[i] = label\n",
    "\n",
    "kfolds_train_indices = []\n",
    "kfolds_val_indices = []\n",
    "    \n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=12345)\n",
    "for train_indices, val_indices in skf.split(X, Y):\n",
    "    kfolds_train_indices.append(train_indices)\n",
    "    kfolds_val_indices.append(val_indices)  \n",
    "    \n",
    "kfold_samplers = []\n",
    "for train_indices, val_indices in zip(kfolds_train_indices, kfolds_val_indices):\n",
    "    kfold_samplers.append({\"train\": SubsetRandomSampler(train_indices), \n",
    "                           \"val\": SubsetRandomSampler(val_indices)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentations\n",
    "\n",
    "mean_val = [0.5] * 3  # RGB\n",
    "std_val = [0.5] * 3  # RGB\n",
    "\n",
    "train_transforms = Compose([\n",
    "    ToNumpy(),\n",
    "    # Geometry\n",
    "    RandomChoice([\n",
    "        RandomAffine(rotation=(-60, 60), scale=(0.95, 1.05), translate=(0.05, 0.05)),\n",
    "        RandomFlip(proba=0.5, mode='h'),\n",
    "        RandomFlip(proba=0.5, mode='v'),        \n",
    "    ]),    \n",
    "    # To Tensor (float, CxHxW, [0.0, 1.0]) + Normalize\n",
    "    ToTensor(),\n",
    "    # Color\n",
    "    ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
    "    Normalize(mean_val, std_val)\n",
    "])\n",
    "  \n",
    "\n",
    "test_transforms = Compose([\n",
    "    ToNumpy(),    \n",
    "    # Geometry\n",
    "    RandomChoice([\n",
    "        RandomAffine(rotation=(-60, 60), scale=(0.95, 1.05), translate=(0.05, 0.05)),\n",
    "        RandomFlip(proba=0.5, mode='h'),\n",
    "        RandomFlip(proba=0.5, mode='v'),        \n",
    "    ]),        \n",
    "    # To Tensor (float, CxHxW, [0.0, 1.0])  + Normalize\n",
    "    ToTensor(),\n",
    "    # Color\n",
    "    ColorJitter(brightness=0.15, contrast=0.15, saturation=0.15),\n",
    "    Normalize(mean_val, std_val)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_aug_train_ds = TransformedDataset(resized_train_ds, x_transforms=train_transforms)\n",
    "data_aug_val_ds = TransformedDataset(resized_train_ds, x_transforms=test_transforms)\n",
    "data_aug_test_ds = TransformedDataset(resized_test_ds, x_transforms=test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_index = 0 \n",
    "\n",
    "cuda_train_batches_ds = OnGPUDataLoader(data_aug_train_ds, \n",
    "                                         batch_size=64, \n",
    "                                         sampler=kfold_samplers[split_index][\"train\"], \n",
    "                                         num_workers=4, \n",
    "                                         drop_last=True, \n",
    "                                         pin_memory=True)\n",
    "\n",
    "cuda_val_batches_ds = OnGPUDataLoader(data_aug_val_ds, \n",
    "                                       batch_size=64, \n",
    "                                       sampler=kfold_samplers[split_index][\"val\"], \n",
    "                                       num_workers=4, \n",
    "                                       drop_last=True)\n",
    "\n",
    "cuda_test_batches_ds = OnGPUDataLoader(data_aug_test_ds, \n",
    "                                       batch_size=64, \n",
    "                                       num_workers=4, \n",
    "                                       drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check again training classes distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:08<00:00, 71.28it/s, c0: 15 | c1: 5]]]\n"
     ]
    }
   ],
   "source": [
    "n_classes = 10\n",
    "mean_n_classes = []\n",
    "std_n_classes = []\n",
    "cnt = 0\n",
    "\n",
    "classes_stats_per_batches = np.zeros((len(cuda_train_batches_ds), n_classes), dtype=np.int)\n",
    "\n",
    "pbar = tqdm(total=len(cuda_train_batches_ds))\n",
    "for i, (batch_x, batch_y) in enumerate(cuda_train_batches_ds):\n",
    "    for y in batch_y:\n",
    "        classes_stats_per_batches[i, y] += 1\n",
    "\n",
    "    postfix_str = \"c0: %i | c1: %i\" % (classes_stats_per_batches[i, 0], classes_stats_per_batches[i, 1])\n",
    "    pbar.set_postfix_str(postfix_str, refresh=True)\n",
    "    pbar.update(1)    \n",
    "    \n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fae58313f50>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fae583138d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot(data=classes_stats_per_batches, palette=\"PRGn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fae5037f990>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSqkAiIiklAqAiEhK/RcU1eYn/eNKMwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fae580648d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean_n_classes = np.mean(np.cumsum(classes_stats_per_batches, axis=0), axis=1)\n",
    "std_n_classes = np.std(np.cumsum(classes_stats_per_batches, axis=0), axis=1)\n",
    "\n",
    "plt.plot(mean_n_classes)\n",
    "plt.plot(mean_n_classes + 3.0 * std_n_classes)\n",
    "plt.plot(mean_n_classes - 3.0 * std_n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup model\n",
    "\n",
    "Let's load a small neural network \"SqueezeNet\" that has only ~700K parameters, showing performances similar to AlexNet:\n",
    "- Top-1 ImageNet Accuracy: 57.5% vs 57.2% (AlexNet)\n",
    "- Top-5 ImageNet Accuracy: 80.3% vs 80.3% (AlexNet)\n",
    "\n",
    "References:\n",
    "- [paper](https://arxiv.org/pdf/1602.07360.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import SqueezeNet\n",
    "\n",
    "from common.nn_utils import print_trainable_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SqueezeNet (\n",
      "  (features): Sequential (\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2))\n",
      "    (1): ReLU (inplace)\n",
      "    (2): MaxPool2d (size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
      "    (3): Fire (\n",
      "      (squeeze): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (4): Fire (\n",
      "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (5): MaxPool2d (size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
      "    (6): Fire (\n",
      "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (7): Fire (\n",
      "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (8): MaxPool2d (size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
      "    (9): Fire (\n",
      "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (10): Fire (\n",
      "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (11): Fire (\n",
      "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (12): Fire (\n",
      "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential (\n",
      "    (0): Dropout (p = 0.5)\n",
      "    (1): Conv2d(512, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (2): ReLU (inplace)\n",
      "    (3): AvgPool2d (size=13, stride=13, padding=0, ceil_mode=False, count_include_pad=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "squeezenet = SqueezeNet(num_classes=10, version=1.1)\n",
    "print(squeezenet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('features.0.weight', torch.Size([64, 3, 3, 3]))\n",
      "('features.0.bias', torch.Size([64]))\n",
      "('features.3.squeeze.weight', torch.Size([16, 64, 1, 1]))\n",
      "('features.3.squeeze.bias', torch.Size([16]))\n",
      "('features.3.expand1x1.weight', torch.Size([64, 16, 1, 1]))\n",
      "('features.3.expand1x1.bias', torch.Size([64]))\n",
      "('features.3.expand3x3.weight', torch.Size([64, 16, 3, 3]))\n",
      "('features.3.expand3x3.bias', torch.Size([64]))\n",
      "('features.4.squeeze.weight', torch.Size([16, 128, 1, 1]))\n",
      "('features.4.squeeze.bias', torch.Size([16]))\n",
      "('features.4.expand1x1.weight', torch.Size([64, 16, 1, 1]))\n",
      "('features.4.expand1x1.bias', torch.Size([64]))\n",
      "('features.4.expand3x3.weight', torch.Size([64, 16, 3, 3]))\n",
      "('features.4.expand3x3.bias', torch.Size([64]))\n",
      "('features.6.squeeze.weight', torch.Size([32, 128, 1, 1]))\n",
      "('features.6.squeeze.bias', torch.Size([32]))\n",
      "('features.6.expand1x1.weight', torch.Size([128, 32, 1, 1]))\n",
      "('features.6.expand1x1.bias', torch.Size([128]))\n",
      "('features.6.expand3x3.weight', torch.Size([128, 32, 3, 3]))\n",
      "('features.6.expand3x3.bias', torch.Size([128]))\n",
      "('features.7.squeeze.weight', torch.Size([32, 256, 1, 1]))\n",
      "('features.7.squeeze.bias', torch.Size([32]))\n",
      "('features.7.expand1x1.weight', torch.Size([128, 32, 1, 1]))\n",
      "('features.7.expand1x1.bias', torch.Size([128]))\n",
      "('features.7.expand3x3.weight', torch.Size([128, 32, 3, 3]))\n",
      "('features.7.expand3x3.bias', torch.Size([128]))\n",
      "('features.9.squeeze.weight', torch.Size([48, 256, 1, 1]))\n",
      "('features.9.squeeze.bias', torch.Size([48]))\n",
      "('features.9.expand1x1.weight', torch.Size([192, 48, 1, 1]))\n",
      "('features.9.expand1x1.bias', torch.Size([192]))\n",
      "('features.9.expand3x3.weight', torch.Size([192, 48, 3, 3]))\n",
      "('features.9.expand3x3.bias', torch.Size([192]))\n",
      "('features.10.squeeze.weight', torch.Size([48, 384, 1, 1]))\n",
      "('features.10.squeeze.bias', torch.Size([48]))\n",
      "('features.10.expand1x1.weight', torch.Size([192, 48, 1, 1]))\n",
      "('features.10.expand1x1.bias', torch.Size([192]))\n",
      "('features.10.expand3x3.weight', torch.Size([192, 48, 3, 3]))\n",
      "('features.10.expand3x3.bias', torch.Size([192]))\n",
      "('features.11.squeeze.weight', torch.Size([64, 384, 1, 1]))\n",
      "('features.11.squeeze.bias', torch.Size([64]))\n",
      "('features.11.expand1x1.weight', torch.Size([256, 64, 1, 1]))\n",
      "('features.11.expand1x1.bias', torch.Size([256]))\n",
      "('features.11.expand3x3.weight', torch.Size([256, 64, 3, 3]))\n",
      "('features.11.expand3x3.bias', torch.Size([256]))\n",
      "('features.12.squeeze.weight', torch.Size([64, 512, 1, 1]))\n",
      "('features.12.squeeze.bias', torch.Size([64]))\n",
      "('features.12.expand1x1.weight', torch.Size([256, 64, 1, 1]))\n",
      "('features.12.expand1x1.bias', torch.Size([256]))\n",
      "('features.12.expand3x3.weight', torch.Size([256, 64, 3, 3]))\n",
      "('features.12.expand3x3.bias', torch.Size([256]))\n",
      "('classifier.1.weight', torch.Size([10, 512, 1, 1]))\n",
      "('classifier.1.bias', torch.Size([10]))\n",
      "('\\nTotal number of trainable parameters: ', 727626)\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(squeezenet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put the model on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "squeezenet = squeezenet.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we work with resized CIFAR10 images of size 42 x 42, we replace first layers that drastically reduce feature map size and adapt the last average pooling layer in order to avoid zero size feature maps. \n",
    "\n",
    "Let's see feature map sizes coming from `squeezenet.features`:\n",
    "- from the first layers before the first 'fire' module: \n",
    "    - 0: Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2))\n",
    "    - 1: ReLU (inplace)\n",
    "    - 2: MaxPool2d (size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
    "- the last layer before classification part:\n",
    "    - 12: Fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import AvgPool2d, Sequential, MaxPool2d, Conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2)),\n",
       " MaxPool2d (size=(3, 3), stride=(2, 2), dilation=(1, 1)))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squeezenet.features[0], squeezenet.features[2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 64, 23, 23]), torch.Size([1, 64, 11, 11]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "test_random_x = Variable(torch.randn(1, 3, 48, 48).pin_memory()).cuda(async=True)\n",
    "\n",
    "squeezenet.eval()\n",
    "test_output_y0 = squeezenet.features[0](test_random_x)\n",
    "test_output_y1 = Sequential(squeezenet.features[0], squeezenet.features[1], squeezenet.features[2])(test_random_x)\n",
    "test_output_y0.size(), test_output_y1.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's replace first layers : \n",
    "- `Conv2d (3, 64, kernel_size=(3, 3), stride=(2, 2))` by `Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=1)`\n",
    "- remove `MaxPool2d (size=(3, 3), stride=(2, 2), dilation=(1, 1)))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [l for i, l in enumerate(squeezenet.features) if i != 2]\n",
    "layers[0] = Conv2d(3, 64, kernel_size=(3, 3), padding=1)\n",
    "\n",
    "squeezenet.features = Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SqueezeNet (\n",
      "  (features): Sequential (\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU (inplace)\n",
      "    (2): Fire (\n",
      "      (squeeze): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (3): Fire (\n",
      "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (4): MaxPool2d (size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
      "    (5): Fire (\n",
      "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (6): Fire (\n",
      "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (7): MaxPool2d (size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
      "    (8): Fire (\n",
      "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (9): Fire (\n",
      "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (10): Fire (\n",
      "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (11): Fire (\n",
      "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential (\n",
      "    (0): Dropout (p = 0.5)\n",
      "    (1): Conv2d(512, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (2): ReLU (inplace)\n",
      "    (3): AvgPool2d (size=13, stride=13, padding=0, ceil_mode=False, count_include_pad=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(squeezenet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "squeezenet = squeezenet.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 10, 10])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "test_random_x = Variable(torch.randn(1, 3, 42, 42).pin_memory()).cuda(async=True)\n",
    "\n",
    "squeezenet.eval()\n",
    "test_output_y = squeezenet.features(test_random_x)\n",
    "test_output_y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import AvgPool2d, Sequential\n",
    "\n",
    "layers = [l for l in squeezenet.classifier]\n",
    "layers[-1] = AvgPool2d(10)\n",
    "\n",
    "squeezenet.classifier = Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SqueezeNet (\n",
      "  (features): Sequential (\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU (inplace)\n",
      "    (2): Fire (\n",
      "      (squeeze): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (3): Fire (\n",
      "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (4): MaxPool2d (size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
      "    (5): Fire (\n",
      "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (6): Fire (\n",
      "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (7): MaxPool2d (size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
      "    (8): Fire (\n",
      "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (9): Fire (\n",
      "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (10): Fire (\n",
      "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (11): Fire (\n",
      "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential (\n",
      "    (0): Dropout (p = 0.5)\n",
      "    (1): Conv2d(512, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (2): ReLU (inplace)\n",
      "    (3): AvgPool2d (size=10, stride=10, padding=0, ceil_mode=False, count_include_pad=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(squeezenet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from previous prints, the last layer of the model is average pooling and the output of the forward pass is logits and not yet probabilities of classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.0262  0.0050  0.0538  0.0001  0.0054  0.0026  0.0000  0.1106  0.0109  0.1962\n",
       "[torch.cuda.FloatTensor of size 1x10 (GPU 0)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "test_random_x = Variable(torch.randn(1, 3, 48, 48).pin_memory()).cuda(async=True)\n",
    "\n",
    "squeezenet.eval()\n",
    "test_output_y = squeezenet(test_random_x)\n",
    "test_output_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup loss function and optimizer\n",
    "\n",
    "Let's choose classical cross-entropy loss function for this multiclass classification task and Adam as an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function (criterion) and optimizer\n",
    "criterion = CrossEntropyLoss().cuda()\n",
    "optimizer = Adam(squeezenet.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that CrossEntropyLoss:\n",
    "\n",
    "> This criterion combines `LogSoftMax` and `NLLLoss` in one single class.\n",
    "> It is useful when training a classification problem with `n` classes.\n",
    "\n",
    "> The `input` is expected to contain scores for each class.\n",
    "> `input` has to be a 2D `Tensor` of size `batch x n`.\n",
    "\n",
    "> This criterion expects a class index (0 to nClasses-1) as the\n",
    "> `target` for each value of a 1D tensor of size `n`\n",
    "\n",
    ">    The loss can be described as:\n",
    "$$\n",
    "        loss(x, class) = -log(exp(x[class]) / (\\sum_j exp(x[j])))\n",
    "                       = -x[class] + log(\\sum_j exp(x[j]))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check loss function computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Loss : ', \n",
      " 2.3079\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "squeezenet.eval()\n",
    "\n",
    "for i, (batch_x, batch_y) in enumerate(cuda_train_batches_ds):\n",
    "    \n",
    "    batch_x = Variable(batch_x, requires_grad=True)\n",
    "    batch_y = Variable(batch_y)\n",
    "    batch_y_pred = squeezenet(batch_x)\n",
    "    loss = criterion(batch_y_pred, batch_y)\n",
    "    print(\"Loss : \", loss.data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup training pipeline\n",
    "\n",
    "Typical pipeline: \n",
    "- loops over training dataset during `n_epochs`\n",
    "    - computes values of the loss function, accuracy, other metrics on train batches\n",
    "- runs validation phase on validation dataset when training epoch ends\n",
    "    - computes values of the loss function, accuracy, other metrics on whole validation dataset\n",
    "- save on the disk the best model defined by a metric\n",
    "- performs learning rate scheduling on each training epoch\n",
    "\n",
    "\n",
    "Next, there are two ways:\n",
    "- copy/modify code from examples: i.e. [link](https://github.com/pytorch/examples/blob/master/imagenet/main.py)\n",
    "- use [torchsample](https://github.com/ncullen93/torchsample) or [tnt](https://github.com/pytorch/tnt) ...\n",
    "\n",
    "Here we choose the first way and explicitly code a training pipeline based on various available examples.\n",
    "Let's first define methods called in the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "        \n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, cuda_batches, criterion, optimizer, epoch, n_epochs):    \n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    \n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    pbar = tqdm(total=len(cuda_batches))\n",
    "    for i, (batch_x, batch_y) in enumerate(cuda_batches):\n",
    "\n",
    "        batch_x = Variable(batch_x)\n",
    "        batch_y = Variable(batch_y)\n",
    "\n",
    "        # compute output\n",
    "        batch_y_pred = model(batch_x)\n",
    "        loss = criterion(batch_y_pred, batch_y)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(batch_y_pred.data, batch_y.data, topk=(1, 5))\n",
    "        losses.update(loss.data[0], batch_x.size(0))\n",
    "        top1.update(prec1[0], batch_x.size(0))\n",
    "        top5.update(prec5[0], batch_x.size(0))\n",
    "\n",
    "        # compute gradient and do optimizer step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        prefix_str = \"Epoch: {}/{}\".format(epoch + 1, n_epochs)        \n",
    "        pbar.set_description_str(prefix_str, refresh=False)\n",
    "        \n",
    "        post_fix_str = 'Loss {loss.avg:.4f} | ' + \\\n",
    "                        'Prec@1 {top1.avg:.3f} | ' + \\\n",
    "                        'Prec@5 {top5.avg:.3f}'\n",
    "                        \n",
    "        post_fix_str = post_fix_str.format(loss=losses, top1=top1, top5=top5)\n",
    "        \n",
    "        pbar.set_postfix_str(post_fix_str, refresh=True)\n",
    "        pbar.update(1)\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, cuda_batches, criterion):\n",
    "    \n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    pbar = tqdm(total=len(cuda_batches))\n",
    "    for i, (batch_x, batch_y) in enumerate(cuda_batches):\n",
    "\n",
    "        batch_x = Variable(batch_x, volatile=True)  # volatile means that the Variable should be used in inference mode, i.e. don’t save the history.\n",
    "        batch_y = Variable(batch_y, volatile=True)   # see http://pytorch.org/docs/master/autograd.html#variable\n",
    "\n",
    "        # compute output\n",
    "        batch_y_pred = model(batch_x)\n",
    "        loss = criterion(batch_y_pred, batch_y)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(batch_y_pred.data, batch_y.data, topk=(1, 5))\n",
    "        losses.update(loss.data[0], batch_x.size(0))\n",
    "        top1.update(prec1[0], batch_x.size(0))\n",
    "        top5.update(prec5[0], batch_x.size(0))\n",
    "\n",
    "        pbar.set_description_str(\"Test\", refresh=False)\n",
    "        post_fix_str = 'Loss {loss.avg:.4f} | ' + \\\n",
    "                        'Prec@1 {top1.avg:.3f} | ' + \\\n",
    "                        'Prec@5 {top5.avg:.3f}'\n",
    "        post_fix_str = post_fix_str.format(loss=losses, top1=top1, top5=top5)\n",
    "        \n",
    "        pbar.set_postfix_str(post_fix_str, refresh=True)\n",
    "        pbar.update(1)\n",
    "    pbar.close()                        \n",
    "    return top1.avg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ExponentialLR, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from glob import glob\n",
    "\n",
    "def save_checkpoint(logs_path, state, is_best):\n",
    "    filename='checkpoint_{epoch}_val_prec1={val_prec1:.4f}.pth.tar'.format(\n",
    "        epoch=state['epoch'],\n",
    "        val_prec1=state['val_prec1']\n",
    "    )\n",
    "    torch.save(state, os.path.join(logs_path, filename))\n",
    "    if is_best:        \n",
    "        best_model_filenames = glob(os.path.join(logs_path, 'model_val_prec1*'))\n",
    "        for fn in best_model_filenames:\n",
    "            os.remove(fn)        \n",
    "        best_model_filename='model_val_prec1={val_prec1:.4f}.pth.tar'.format(\n",
    "            val_prec1=state['val_prec1']\n",
    "        )\n",
    "        shutil.copyfile(os.path.join(logs_path, filename), os.path.join(logs_path, best_model_filename))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we can start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0 \n",
    "n_epochs = 10\n",
    "model = squeezenet\n",
    "init_lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.backends import cudnn\n",
    "cudnn.benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What does torch.backends.cudnn.benchmark do? [url](https://discuss.pytorch.org/t/what-does-torch-backends-cudnn-benchmark-do/5936/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(squeezenet.parameters(), lr=init_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10: 100%|██████████| 625/625 [00:34<00:00, 13.84it/s, Loss 1.9842 | Prec@1 23.858 | Prec@5 75.500]\n",
      "Test: 100%|██████████| 156/156 [00:03<00:00, 44.86it/s, Loss 1.8705 | Prec@1 30.148 | Prec@5 82.362]\n",
      "Epoch: 2/10: 100%|██████████| 625/625 [00:34<00:00, 13.00it/s, Loss 1.7278 | Prec@1 35.767 | Prec@5 86.498]\n",
      "Test: 100%|██████████| 156/156 [00:03<00:00, 34.68it/s, Loss 1.7060 | Prec@1 36.569 | Prec@5 87.190]\n",
      "Epoch: 3/10: 100%|██████████| 625/625 [00:34<00:00, 18.13it/s, Loss 1.6277 | Prec@1 39.945 | Prec@5 88.565]\n",
      "Test: 100%|██████████| 156/156 [00:03<00:00, 43.38it/s, Loss 1.5835 | Prec@1 41.937 | Prec@5 89.493]\n",
      "Epoch: 4/10: 100%|██████████| 625/625 [00:34<00:00, 18.22it/s, Loss 1.5775 | Prec@1 42.233 | Prec@5 89.530]\n",
      "Test: 100%|██████████| 156/156 [00:03<00:00, 65.36it/s, Loss 1.5762 | Prec@1 42.899 | Prec@5 89.543]\n",
      "Epoch: 5/10: 100%|██████████| 625/625 [00:35<00:00, 11.48it/s, Loss 1.5284 | Prec@1 44.337 | Prec@5 90.118]\n",
      "Test: 100%|██████████| 156/156 [00:03<00:00, 44.01it/s, Loss 1.5440 | Prec@1 43.660 | Prec@5 90.054]\n",
      "Epoch: 6/10: 100%|██████████| 625/625 [00:34<00:00, 18.48it/s, Loss 1.4897 | Prec@1 46.000 | Prec@5 90.920]\n",
      "Test: 100%|██████████| 156/156 [00:03<00:00, 41.81it/s, Loss 1.5153 | Prec@1 45.062 | Prec@5 91.326]\n",
      "Epoch: 7/10: 100%|██████████| 625/625 [00:34<00:00, 17.88it/s, Loss 1.4594 | Prec@1 46.888 | Prec@5 91.332]\n",
      "Test: 100%|██████████| 156/156 [00:03<00:00, 44.02it/s, Loss 1.5265 | Prec@1 45.092 | Prec@5 91.116]\n",
      "Epoch: 8/10: 100%|██████████| 625/625 [00:34<00:00, 18.30it/s, Loss 1.4109 | Prec@1 48.745 | Prec@5 92.380]\n",
      "Test: 100%|██████████| 156/156 [00:03<00:00, 45.78it/s, Loss 1.4291 | Prec@1 48.538 | Prec@5 91.997]\n",
      "Epoch: 9/10: 100%|██████████| 625/625 [00:34<00:00, 18.22it/s, Loss 1.3818 | Prec@1 49.940 | Prec@5 92.630]\n",
      "Test: 100%|██████████| 156/156 [00:03<00:00, 42.92it/s, Loss 1.3693 | Prec@1 50.871 | Prec@5 92.989]\n",
      "Epoch: 10/10: 100%|██████████| 625/625 [00:35<00:00, 12.48it/s, Loss 1.3586 | Prec@1 50.960 | Prec@5 92.990]\n",
      "Test: 100%|██████████| 156/156 [00:03<00:00, 42.86it/s, Loss 1.3952 | Prec@1 49.519 | Prec@5 92.448]\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "best_prec1 = 0\n",
    "now = datetime.now()\n",
    "\n",
    "if not os.path.exists('logs'):\n",
    "    os.makedirs('logs')\n",
    "    \n",
    "logs_path = os.path.join('logs', 'cifar10_squeezenet_%s' % now.strftime(\"%Y%m%d_%H%M\"))\n",
    "if not os.path.exists(logs_path):\n",
    "    os.makedirs(logs_path)    \n",
    "\n",
    "\n",
    "for epoch in range(start_epoch, n_epochs):\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "    # train for one epoch\n",
    "    train_one_epoch(model, cuda_train_batches_ds, criterion, optimizer, epoch, n_epochs)\n",
    "\n",
    "    # evaluate on validation set\n",
    "    prec1 = validate(model, cuda_val_batches_ds, criterion)\n",
    "\n",
    "    # remember best prec@1 and save checkpoint\n",
    "    is_best = prec1 > best_prec1\n",
    "    best_prec1 = max(prec1, best_prec1)\n",
    "    save_checkpoint(logs_path, {\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'val_prec1': prec1,\n",
    "        'best_prec1': best_prec1,\n",
    "        'optimizer' : optimizer.state_dict()}, is_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "Let's upload the best saved model and run inference on the testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_filenames = glob(os.path.join(logs_path, \"model_val_prec1=*\"))\n",
    "assert len(saved_model_filenames) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'logs/cifar10_squeezenet_20171113_1830/model_val_prec1=50.8714.pth.tar'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_model_filename = saved_model_filenames[0]\n",
    "saved_model_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_squeezenet_state_dict = torch.load(saved_model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "squeezenet.load_state_dict(pretrained_squeezenet_state_dict['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 156/156 [00:02<00:00, 52.31it/s, Loss 1.3473 | Prec@1 51.482 | Prec@5 93.199]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "51.482371794871796"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate on validation set\n",
    "prec1 = validate(squeezenet, cuda_test_batches_ds, criterion)\n",
    "prec1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
