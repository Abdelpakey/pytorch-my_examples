{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR10 Classifier. Training\n",
    "\n",
    "\n",
    "This is the third part of the tutorial on how to train a classifier on CIFAR10 dataset. \n",
    "\n",
    "Here we will assemble the results of two previous parts and train a model on CIFAR10.\n",
    "\n",
    "- Setup dataflow\n",
    "- Setup model: SqueezeNet v1.1\n",
    "- Setup loss function and optimizer\n",
    "- Setup training pipeline\n",
    "    - Training\n",
    "    - Inference\n",
    "\n",
    "References:\n",
    "- [pytorch-examples/imagenet](https://github.com/pytorch/examples/blob/master/imagenet/main.py)\n",
    "- [pytorch-transfer learning](http://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html#sphx-glr-beginner-transfer-learning-tutorial-py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://ipython.org/ipython-doc/3/config/extensions/autoreload.html\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CIFAR10_ROOT'] = '/media/user/fast_storage/tensorpack_data/cifar10_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CIFAR10_ROOT = os.environ['CIFAR10_ROOT']\n",
    "sys.path.append(\"common\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup dataflow\n",
    "\n",
    "Again, we have two CIFAR10 datasets: `training` and `testing`. Next, we :\n",
    "- separate training dataset using stratified split into `n` folds of `training` and `validation` datasets.\n",
    "- apply data augmentations\n",
    "- gather data in batches\n",
    "- all previous operations using multiprocessing\n",
    "- load on GPU\n",
    "\n",
    "We won't iterate over folds and consider only the first fold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, Normalize, ToTensor, Lambda\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from common.dataflow import TransformedDataset, OnCudaDataLoader\n",
    "from common.imgaug import ToNumpy, RandomOrder, RandomChoice, RandomFlip, RandomAffine, RandomAdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw datasets: training and testing\n",
    "train_ds = torchvision.datasets.CIFAR10(root=CIFAR10_ROOT, train=True, download=False)\n",
    "test_ds = torchvision.datasets.CIFAR10(root=CIFAR10_ROOT, train=False, download=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's again resize input images to 42x42. Most of common state-of-the-art architectures requires input images larger than 224x224. Smaller input images will produce zero-size feature maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ResizeDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, ds, output_size=(32, 32)):        \n",
    "        assert isinstance(ds, Dataset)        \n",
    "        self.ds = ds\n",
    "        self.output_size = output_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.ds[index]\n",
    "        x = x.resize(self.output_size)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "resized_train_ds = ResizeDataset(train_ds, output_size=(42, 42))\n",
    "resized_test_ds = ResizeDataset(test_ds, output_size=(42, 42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "n_samples = len(resized_train_ds)\n",
    "X = np.zeros(n_samples)\n",
    "Y = np.zeros(n_samples)\n",
    "for i, (_, label) in enumerate(resized_train_ds):\n",
    "    Y[i] = label\n",
    "\n",
    "kfolds_train_indices = []\n",
    "kfolds_val_indices = []\n",
    "    \n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=12345)\n",
    "for train_indices, val_indices in skf.split(X, Y):\n",
    "    kfolds_train_indices.append(train_indices)\n",
    "    kfolds_val_indices.append(val_indices)  \n",
    "    \n",
    "kfold_samplers = []\n",
    "for train_indices, val_indices in zip(kfolds_train_indices, kfolds_val_indices):\n",
    "    kfold_samplers.append({\"train\": SubsetRandomSampler(train_indices), \n",
    "                           \"val\": SubsetRandomSampler(val_indices)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentations\n",
    "\n",
    "mean_val = [0.5] * 3  # RGB\n",
    "std_val = [0.5] * 3  # RGB\n",
    "\n",
    "train_transforms = Compose([\n",
    "    ToNumpy(),\n",
    "    # Geometry\n",
    "    RandomChoice([\n",
    "        RandomAffine(rotation=(-60, 60), scale=(0.95, 1.05), translate=(0.05, 0.05)),\n",
    "        RandomFlip(proba=0.5, mode='h'),\n",
    "        RandomFlip(proba=0.5, mode='v'),        \n",
    "    ]),    \n",
    "    # Color\n",
    "    RandomChoice([\n",
    "       RandomAdd(value=(-10, 10), per_channel=0),\n",
    "       RandomAdd(value=(-10, 10), per_channel=1),\n",
    "       RandomAdd(value=(-10, 10), per_channel=2),\n",
    "       RandomAdd(value=(-10, 10))\n",
    "    ]),\n",
    "    # To Tensor (float, CxHxW, [0.0, 1.0]) + Normalize\n",
    "    ToTensor(),\n",
    "    Normalize(mean_val, std_val)\n",
    "])\n",
    "  \n",
    "\n",
    "test_transforms = Compose([\n",
    "    ToNumpy(),    \n",
    "    # Geometry\n",
    "    RandomChoice([\n",
    "        RandomAffine(rotation=(-60, 60), scale=(0.95, 1.05), translate=(0.05, 0.05)),\n",
    "        RandomFlip(proba=0.5, mode='h'),\n",
    "        RandomFlip(proba=0.5, mode='v'),        \n",
    "    ]),        \n",
    "    # Color\n",
    "    RandomAdd(value=(-10, 10)),\n",
    "    # To Tensor (float, CxHxW, [0.0, 1.0])  + Normalize\n",
    "    ToTensor(),\n",
    "    Normalize(mean_val, std_val)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_aug_train_ds = TransformedDataset(resized_train_ds, x_transforms=train_transforms)\n",
    "data_aug_val_ds = TransformedDataset(resized_train_ds, x_transforms=test_transforms)\n",
    "data_aug_test_ds = TransformedDataset(resized_test_ds, x_transforms=test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_index = 0 \n",
    "\n",
    "cuda_train_batches_ds = OnCudaDataLoader(data_aug_train_ds, \n",
    "                                         batch_size=64, \n",
    "                                         sampler=kfold_samplers[split_index][\"train\"], \n",
    "                                         num_workers=4, \n",
    "                                         drop_last=True, \n",
    "                                         pin_memory=True)\n",
    "\n",
    "cuda_val_batches_ds = OnCudaDataLoader(data_aug_val_ds, \n",
    "                                       batch_size=64, \n",
    "                                       sampler=kfold_samplers[split_index][\"val\"], \n",
    "                                       num_workers=4, \n",
    "                                       drop_last=True)\n",
    "\n",
    "cuda_test_batches_ds = OnCudaDataLoader(data_aug_test_ds, \n",
    "                                       batch_size=64, \n",
    "                                       num_workers=4, \n",
    "                                       drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check again training classes distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:04<00:00, 151.52it/s, c0: 6 | c1: 2]]]\n"
     ]
    }
   ],
   "source": [
    "n_classes = 10\n",
    "mean_n_classes = []\n",
    "std_n_classes = []\n",
    "cnt = 0\n",
    "\n",
    "classes_stats_per_batches = np.zeros((len(cuda_train_batches_ds), n_classes), dtype=np.int)\n",
    "\n",
    "pbar = tqdm(total=len(cuda_train_batches_ds))\n",
    "for i, (batch_x, batch_y) in enumerate(cuda_train_batches_ds):\n",
    "    for y in batch_y:\n",
    "        classes_stats_per_batches[i, y] += 1\n",
    "\n",
    "    postfix_str = \"c0: %i | c1: %i\" % (classes_stats_per_batches[i, 0], classes_stats_per_batches[i, 1])\n",
    "    pbar.set_postfix_str(postfix_str, refresh=True)\n",
    "    pbar.update(1)    \n",
    "    \n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fb9c3ffae48>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb93a368ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot(data=classes_stats_per_batches, palette=\"PRGn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb9bc7dcfd0>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb93a10ccf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean_n_classes = np.mean(np.cumsum(classes_stats_per_batches, axis=0), axis=1)\n",
    "std_n_classes = np.std(np.cumsum(classes_stats_per_batches, axis=0), axis=1)\n",
    "\n",
    "plt.plot(mean_n_classes)\n",
    "plt.plot(mean_n_classes + 3.0 * std_n_classes)\n",
    "plt.plot(mean_n_classes - 3.0 * std_n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup model\n",
    "\n",
    "Let's load a small neural network \"SqueezeNet\" that has only ~700K parameters, showing performances similar to AlexNet:\n",
    "- Top-1 ImageNet Accuracy: 57.5% vs 57.2% (AlexNet)\n",
    "- Top-5 ImageNet Accuracy: 80.3% vs 80.3% (AlexNet)\n",
    "\n",
    "References:\n",
    "- [paper](https://arxiv.org/pdf/1602.07360.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import SqueezeNet\n",
    "\n",
    "from common.nn_utils import print_trainable_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SqueezeNet (\n",
      "  (features): Sequential (\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2))\n",
      "    (1): ReLU (inplace)\n",
      "    (2): MaxPool2d (size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
      "    (3): Fire (\n",
      "      (squeeze): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (4): Fire (\n",
      "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (5): MaxPool2d (size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
      "    (6): Fire (\n",
      "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (7): Fire (\n",
      "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (8): MaxPool2d (size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
      "    (9): Fire (\n",
      "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (10): Fire (\n",
      "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (11): Fire (\n",
      "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (12): Fire (\n",
      "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential (\n",
      "    (0): Dropout (p = 0.5)\n",
      "    (1): Conv2d(512, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (2): ReLU (inplace)\n",
      "    (3): AvgPool2d (size=13, stride=13, padding=0, ceil_mode=False, count_include_pad=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "squeezenet = SqueezeNet(num_classes=10, version=1.1)\n",
    "print(squeezenet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0.weight torch.Size([64, 3, 3, 3])\n",
      "features.0.bias torch.Size([64])\n",
      "features.3.squeeze.weight torch.Size([16, 64, 1, 1])\n",
      "features.3.squeeze.bias torch.Size([16])\n",
      "features.3.expand1x1.weight torch.Size([64, 16, 1, 1])\n",
      "features.3.expand1x1.bias torch.Size([64])\n",
      "features.3.expand3x3.weight torch.Size([64, 16, 3, 3])\n",
      "features.3.expand3x3.bias torch.Size([64])\n",
      "features.4.squeeze.weight torch.Size([16, 128, 1, 1])\n",
      "features.4.squeeze.bias torch.Size([16])\n",
      "features.4.expand1x1.weight torch.Size([64, 16, 1, 1])\n",
      "features.4.expand1x1.bias torch.Size([64])\n",
      "features.4.expand3x3.weight torch.Size([64, 16, 3, 3])\n",
      "features.4.expand3x3.bias torch.Size([64])\n",
      "features.6.squeeze.weight torch.Size([32, 128, 1, 1])\n",
      "features.6.squeeze.bias torch.Size([32])\n",
      "features.6.expand1x1.weight torch.Size([128, 32, 1, 1])\n",
      "features.6.expand1x1.bias torch.Size([128])\n",
      "features.6.expand3x3.weight torch.Size([128, 32, 3, 3])\n",
      "features.6.expand3x3.bias torch.Size([128])\n",
      "features.7.squeeze.weight torch.Size([32, 256, 1, 1])\n",
      "features.7.squeeze.bias torch.Size([32])\n",
      "features.7.expand1x1.weight torch.Size([128, 32, 1, 1])\n",
      "features.7.expand1x1.bias torch.Size([128])\n",
      "features.7.expand3x3.weight torch.Size([128, 32, 3, 3])\n",
      "features.7.expand3x3.bias torch.Size([128])\n",
      "features.9.squeeze.weight torch.Size([48, 256, 1, 1])\n",
      "features.9.squeeze.bias torch.Size([48])\n",
      "features.9.expand1x1.weight torch.Size([192, 48, 1, 1])\n",
      "features.9.expand1x1.bias torch.Size([192])\n",
      "features.9.expand3x3.weight torch.Size([192, 48, 3, 3])\n",
      "features.9.expand3x3.bias torch.Size([192])\n",
      "features.10.squeeze.weight torch.Size([48, 384, 1, 1])\n",
      "features.10.squeeze.bias torch.Size([48])\n",
      "features.10.expand1x1.weight torch.Size([192, 48, 1, 1])\n",
      "features.10.expand1x1.bias torch.Size([192])\n",
      "features.10.expand3x3.weight torch.Size([192, 48, 3, 3])\n",
      "features.10.expand3x3.bias torch.Size([192])\n",
      "features.11.squeeze.weight torch.Size([64, 384, 1, 1])\n",
      "features.11.squeeze.bias torch.Size([64])\n",
      "features.11.expand1x1.weight torch.Size([256, 64, 1, 1])\n",
      "features.11.expand1x1.bias torch.Size([256])\n",
      "features.11.expand3x3.weight torch.Size([256, 64, 3, 3])\n",
      "features.11.expand3x3.bias torch.Size([256])\n",
      "features.12.squeeze.weight torch.Size([64, 512, 1, 1])\n",
      "features.12.squeeze.bias torch.Size([64])\n",
      "features.12.expand1x1.weight torch.Size([256, 64, 1, 1])\n",
      "features.12.expand1x1.bias torch.Size([256])\n",
      "features.12.expand3x3.weight torch.Size([256, 64, 3, 3])\n",
      "features.12.expand3x3.bias torch.Size([256])\n",
      "classifier.1.weight torch.Size([10, 512, 1, 1])\n",
      "classifier.1.bias torch.Size([10])\n",
      "\n",
      "Total number of trainable parameters:  727626\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(squeezenet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put the model on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "squeezenet = squeezenet.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we work with resized CIFAR10 images of size 42 x 42, we replace first layers that drastically reduce feature map size and adapt the last average pooling layer in order to avoid zero size feature maps. \n",
    "\n",
    "Let's see feature map sizes coming from `squeezenet.features`:\n",
    "- from the first layers before the first 'fire' module: \n",
    "    - 0: Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2))\n",
    "    - 1: ReLU (inplace)\n",
    "    - 2: MaxPool2d (size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
    "- the last layer before classification part:\n",
    "    - 12: Fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import AvgPool2d, Sequential, MaxPool2d, Conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2)),\n",
       " MaxPool2d (size=(3, 3), stride=(2, 2), dilation=(1, 1)))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squeezenet.features[0], squeezenet.features[2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 64, 23, 23]), torch.Size([1, 64, 11, 11]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "test_random_x = Variable(torch.randn(1, 3, 48, 48).pin_memory()).cuda(async=True)\n",
    "\n",
    "squeezenet.eval()\n",
    "test_output_y0 = squeezenet.features[0](test_random_x)\n",
    "test_output_y1 = Sequential(squeezenet.features[0], squeezenet.features[1], squeezenet.features[2])(test_random_x)\n",
    "test_output_y0.size(), test_output_y1.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's replace first layers : \n",
    "- `Conv2d (3, 64, kernel_size=(3, 3), stride=(2, 2))` by `Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=1)`\n",
    "- remove `MaxPool2d (size=(3, 3), stride=(2, 2), dilation=(1, 1)))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [l for i, l in enumerate(squeezenet.features) if i != 2]\n",
    "layers[0] = Conv2d(3, 64, kernel_size=(3, 3), padding=1)\n",
    "\n",
    "squeezenet.features = Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SqueezeNet (\n",
      "  (features): Sequential (\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU (inplace)\n",
      "    (2): Fire (\n",
      "      (squeeze): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (3): Fire (\n",
      "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (4): MaxPool2d (size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
      "    (5): Fire (\n",
      "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (6): Fire (\n",
      "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (7): MaxPool2d (size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
      "    (8): Fire (\n",
      "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (9): Fire (\n",
      "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (10): Fire (\n",
      "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (11): Fire (\n",
      "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential (\n",
      "    (0): Dropout (p = 0.5)\n",
      "    (1): Conv2d(512, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (2): ReLU (inplace)\n",
      "    (3): AvgPool2d (size=13, stride=13, padding=0, ceil_mode=False, count_include_pad=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(squeezenet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "squeezenet = squeezenet.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 10, 10])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "test_random_x = Variable(torch.randn(1, 3, 42, 42).pin_memory()).cuda(async=True)\n",
    "\n",
    "squeezenet.eval()\n",
    "test_output_y = squeezenet.features(test_random_x)\n",
    "test_output_y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import AvgPool2d, Sequential\n",
    "\n",
    "layers = [l for l in squeezenet.classifier]\n",
    "layers[-1] = AvgPool2d(10)\n",
    "\n",
    "squeezenet.classifier = Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SqueezeNet (\n",
      "  (features): Sequential (\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU (inplace)\n",
      "    (2): Fire (\n",
      "      (squeeze): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (3): Fire (\n",
      "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (4): MaxPool2d (size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
      "    (5): Fire (\n",
      "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (6): Fire (\n",
      "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (7): MaxPool2d (size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
      "    (8): Fire (\n",
      "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (9): Fire (\n",
      "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (10): Fire (\n",
      "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "    (11): Fire (\n",
      "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU (inplace)\n",
      "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU (inplace)\n",
      "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU (inplace)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential (\n",
      "    (0): Dropout (p = 0.5)\n",
      "    (1): Conv2d(512, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (2): ReLU (inplace)\n",
      "    (3): AvgPool2d (size=10, stride=10, padding=0, ceil_mode=False, count_include_pad=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(squeezenet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from previous prints, the last layer of the model is average pooling and the output of the forward pass is logits and not yet probabilities of classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.0000  0.0044  0.0480  0.0773  0.4036  0.0285  0.0000  0.0144  0.1189  0.1695\n",
       "[torch.cuda.FloatTensor of size 1x10 (GPU 0)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "test_random_x = Variable(torch.randn(1, 3, 48, 48).pin_memory()).cuda(async=True)\n",
    "\n",
    "squeezenet.eval()\n",
    "test_output_y = squeezenet(test_random_x)\n",
    "test_output_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup loss function and optimizer\n",
    "\n",
    "Let's choose classical cross-entropy loss function for this multiclass classification task and Adam as an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function (criterion) and optimizer\n",
    "criterion = CrossEntropyLoss().cuda()\n",
    "optimizer = Adam(squeezenet.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that CrossEntropyLoss:\n",
    "\n",
    "> This criterion combines `LogSoftMax` and `NLLLoss` in one single class.\n",
    "> It is useful when training a classification problem with `n` classes.\n",
    "\n",
    "> The `input` is expected to contain scores for each class.\n",
    "> `input` has to be a 2D `Tensor` of size `batch x n`.\n",
    "\n",
    "> This criterion expects a class index (0 to nClasses-1) as the\n",
    "> `target` for each value of a 1D tensor of size `n`\n",
    "\n",
    ">    The loss can be described as:\n",
    "$$\n",
    "        loss(x, class) = -log(exp(x[class]) / (\\sum_j exp(x[j])))\n",
    "                       = -x[class] + log(\\sum_j exp(x[j]))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check loss function computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss :  \n",
      " 2.3042\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "squeezenet.eval()\n",
    "\n",
    "for i, (batch_x, batch_y) in enumerate(cuda_train_batches_ds):\n",
    "    \n",
    "    batch_x = Variable(batch_x, requires_grad=True)\n",
    "    batch_y = Variable(batch_y)\n",
    "    batch_y_pred = squeezenet(batch_x)\n",
    "    loss = criterion(batch_y_pred, batch_y)\n",
    "    print(\"Loss : \", loss.data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup training pipeline\n",
    "\n",
    "Typical pipeline: \n",
    "- loops over training dataset during `n_epochs`\n",
    "    - computes values of the loss function, accuracy, other metrics on train batches\n",
    "- runs validation phase on validation dataset when training epoch ends\n",
    "    - computes values of the loss function, accuracy, other metrics on whole validation dataset\n",
    "- save on the disk the best model defined by a metric\n",
    "- performs learning rate scheduling on each training epoch\n",
    "\n",
    "\n",
    "Next, there are two ways:\n",
    "- copy/modify code from examples: i.e. [link](https://github.com/pytorch/examples/blob/master/imagenet/main.py)\n",
    "- use [torchsample](https://github.com/ncullen93/torchsample) or [tnt](https://github.com/pytorch/tnt) ...\n",
    "\n",
    "Here we choose the first way and explicitly code a training pipeline based on various available examples.\n",
    "Let's first define methods called in the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "        \n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, cuda_batches, criterion, optimizer, epoch, n_epochs):    \n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    \n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    pbar = tqdm(total=len(cuda_batches))\n",
    "    for i, (batch_x, batch_y) in enumerate(cuda_batches):\n",
    "\n",
    "        batch_x = Variable(batch_x)\n",
    "        batch_y = Variable(batch_y)\n",
    "\n",
    "        # compute output\n",
    "        batch_y_pred = model(batch_x)\n",
    "        loss = criterion(batch_y_pred, batch_y)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(batch_y_pred.data, batch_y.data, topk=(1, 5))\n",
    "        losses.update(loss.data[0], batch_x.size(0))\n",
    "        top1.update(prec1[0], batch_x.size(0))\n",
    "        top5.update(prec5[0], batch_x.size(0))\n",
    "\n",
    "        # compute gradient and do optimizer step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        prefix_str = \"Epoch: {}/{}\".format(epoch + 1, n_epochs)        \n",
    "        pbar.set_description_str(prefix_str, refresh=False)\n",
    "        \n",
    "        post_fix_str = 'Loss {loss.avg:.4f} | ' + \\\n",
    "                        'Prec@1 {top1.avg:.3f} | ' + \\\n",
    "                        'Prec@5 {top5.avg:.3f}'\n",
    "                        \n",
    "        post_fix_str = post_fix_str.format(loss=losses, top1=top1, top5=top5)\n",
    "        \n",
    "        pbar.set_postfix_str(post_fix_str, refresh=True)\n",
    "        pbar.update(1)\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, cuda_batches, criterion):\n",
    "    \n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    pbar = tqdm(total=len(cuda_batches))\n",
    "    for i, (batch_x, batch_y) in enumerate(cuda_batches):\n",
    "\n",
    "        batch_x = Variable(batch_x, volatile=True)  # volatile means that the Variable should be used in inference mode, i.e. don’t save the history.\n",
    "        batch_y = Variable(batch_y, volatile=True)   # see http://pytorch.org/docs/master/autograd.html#variable\n",
    "\n",
    "        # compute output\n",
    "        batch_y_pred = model(batch_x)\n",
    "        loss = criterion(batch_y_pred, batch_y)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(batch_y_pred.data, batch_y.data, topk=(1, 5))\n",
    "        losses.update(loss.data[0], batch_x.size(0))\n",
    "        top1.update(prec1[0], batch_x.size(0))\n",
    "        top5.update(prec5[0], batch_x.size(0))\n",
    "\n",
    "        pbar.set_description_str(\"Test\", refresh=False)\n",
    "        post_fix_str = 'Loss {loss.avg:.4f} | ' + \\\n",
    "                        'Prec@1 {top1.avg:.3f} | ' + \\\n",
    "                        'Prec@5 {top5.avg:.3f}'\n",
    "        post_fix_str = post_fix_str.format(loss=losses, top1=top1, top5=top5)\n",
    "        \n",
    "        pbar.set_postfix_str(post_fix_str, refresh=True)\n",
    "        pbar.update(1)\n",
    "    pbar.close()                        \n",
    "    return top1.avg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ExponentialLR, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from glob import glob\n",
    "\n",
    "def save_checkpoint(logs_path, state, is_best):\n",
    "    filename='checkpoint_{epoch}_val_prec1={val_prec1:.4f}.pth.tar'.format(\n",
    "        epoch=state['epoch'],\n",
    "        val_prec1=state['val_prec1']\n",
    "    )\n",
    "    torch.save(state, os.path.join(logs_path, filename))\n",
    "    if is_best:        \n",
    "        best_model_filenames = glob(os.path.join(logs_path, 'model_val_prec1*'))\n",
    "        for fn in best_model_filenames:\n",
    "            os.remove(fn)        \n",
    "        best_model_filename='model_val_prec1={val_prec1:.4f}.pth.tar'.format(\n",
    "            val_prec1=state['val_prec1']\n",
    "        )\n",
    "        shutil.copyfile(os.path.join(logs_path, filename), os.path.join(logs_path, best_model_filename))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we can start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0 \n",
    "n_epochs = 10\n",
    "model = squeezenet\n",
    "init_lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.backends import cudnn\n",
    "cudnn.benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What does torch.backends.cudnn.benchmark do? [url](https://discuss.pytorch.org/t/what-does-torch-backends-cudnn-benchmark-do/5936/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(squeezenet.parameters(), lr=init_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10: 100%|██████████| 625/625 [00:18<00:00, 34.46it/s, Loss 1.9509 | Prec@1 25.335 | Prec@5 78.703]\n",
      "Test: 100%|██████████| 156/156 [00:01<00:00, 91.37it/s, Loss 1.8145 | Prec@1 33.143 | Prec@5 84.125]\n",
      "Epoch: 1/10: 100%|██████████| 625/625 [00:17<00:00, 34.88it/s, Loss 1.7188 | Prec@1 35.470 | Prec@5 86.985]\n",
      "Test: 100%|██████████| 156/156 [00:01<00:00, 88.10it/s, Loss 1.7030 | Prec@1 37.971 | Prec@5 87.911]\n",
      "Epoch: 2/10: 100%|██████████| 625/625 [00:18<00:00, 35.25it/s, Loss 1.6187 | Prec@1 40.267 | Prec@5 88.778]\n",
      "Test: 100%|██████████| 156/156 [00:01<00:00, 90.00it/s, Loss 1.6275 | Prec@1 40.014 | Prec@5 88.922]\n",
      "Epoch: 3/10: 100%|██████████| 625/625 [00:17<00:00, 35.45it/s, Loss 1.5425 | Prec@1 43.502 | Prec@5 90.095]\n",
      "Test: 100%|██████████| 156/156 [00:01<00:00, 90.63it/s, Loss 1.5587 | Prec@1 42.548 | Prec@5 89.974]\n",
      "Epoch: 4/10: 100%|██████████| 625/625 [00:18<00:00, 34.93it/s, Loss 1.5010 | Prec@1 45.040 | Prec@5 90.672]\n",
      "Test: 100%|██████████| 156/156 [00:01<00:00, 88.17it/s, Loss 1.5449 | Prec@1 44.181 | Prec@5 89.934]\n",
      "Epoch: 5/10: 100%|██████████| 625/625 [00:18<00:00, 35.30it/s, Loss 1.4510 | Prec@1 47.315 | Prec@5 91.420]\n",
      "Test: 100%|██████████| 156/156 [00:01<00:00, 89.12it/s, Loss 1.4524 | Prec@1 46.835 | Prec@5 91.687]\n",
      "Epoch: 6/10: 100%|██████████| 625/625 [00:18<00:00, 35.07it/s, Loss 1.4167 | Prec@1 48.510 | Prec@5 91.950]\n",
      "Test: 100%|██████████| 156/156 [00:01<00:00, 87.78it/s, Loss 1.4332 | Prec@1 47.586 | Prec@5 91.857]\n",
      "Epoch: 7/10: 100%|██████████| 625/625 [00:18<00:00, 35.13it/s, Loss 1.3777 | Prec@1 50.215 | Prec@5 92.645]\n",
      "Test: 100%|██████████| 156/156 [00:01<00:00, 89.50it/s, Loss 1.3959 | Prec@1 49.830 | Prec@5 92.418]\n",
      "Epoch: 8/10: 100%|██████████| 625/625 [00:18<00:00, 34.86it/s, Loss 1.3447 | Prec@1 51.275 | Prec@5 92.930]\n",
      "Test: 100%|██████████| 156/156 [00:01<00:00, 89.55it/s, Loss 1.3567 | Prec@1 51.262 | Prec@5 92.899]\n",
      "Epoch: 9/10: 100%|██████████| 625/625 [00:18<00:00, 34.71it/s, Loss 1.3182 | Prec@1 52.505 | Prec@5 93.267]\n",
      "Test: 100%|██████████| 156/156 [00:01<00:00, 88.67it/s, Loss 1.3178 | Prec@1 52.704 | Prec@5 93.329]\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "best_prec1 = 0\n",
    "now = datetime.now()\n",
    "logs_path = 'logs_cifar10_squeezenet_%s' % now.strftime(\"%Y%m%d_%H%M\")\n",
    "if not os.path.exists(logs_path):\n",
    "    os.makedirs(logs_path)    \n",
    "\n",
    "\n",
    "for epoch in range(start_epoch, n_epochs):\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "    # train for one epoch\n",
    "    train_one_epoch(model, cuda_train_batches_ds, criterion, optimizer, epoch, n_epochs)\n",
    "\n",
    "    # evaluate on validation set\n",
    "    prec1 = validate(model, cuda_val_batches_ds, criterion)\n",
    "\n",
    "    # remember best prec@1 and save checkpoint\n",
    "    is_best = prec1 > best_prec1\n",
    "    best_prec1 = max(prec1, best_prec1)\n",
    "    save_checkpoint(logs_path, {\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'val_prec1': prec1,\n",
    "        'best_prec1': best_prec1,\n",
    "        'optimizer' : optimizer.state_dict()}, is_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "Let's upload the best saved model and run inference on the testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_filenames = glob(os.path.join(logs_path, \"model_val_prec1=*\"))\n",
    "assert len(saved_model_filenames) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'logs_cifar10_squeezenet_20171022_1556/model_val_prec1=52.7043.pth.tar'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_model_filename = saved_model_filenames[0]\n",
    "saved_model_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_squeezenet_state_dict = torch.load(saved_model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "squeezenet.load_state_dict(pretrained_squeezenet_state_dict['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 156/156 [00:01<00:00, 88.21it/s, Loss 1.3090 | Prec@1 52.774 | Prec@5 93.700]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "52.7744391025641"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate on validation set\n",
    "prec1 = validate(squeezenet, cuda_test_batches_ds, criterion)\n",
    "prec1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
